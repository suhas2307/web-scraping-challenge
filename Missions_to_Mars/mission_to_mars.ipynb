{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a funtion that returns the Soup object\n",
    "def create_soup(html):\n",
    "    return bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns Browser object\n",
    "def init_browser():\n",
    "    executable_path = {'executable_path':'/usr/local/bin/chromedriver'}\n",
    "    return Browser('chrome',**executable_path,headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Nasa Mars News\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "# Create browser object\n",
    "browser = init_browser()\n",
    "browser.visit(url)\n",
    "time.sleep(2)\n",
    "# Create Soup object\n",
    "html = browser.html\n",
    "soup = create_soup(html)\n",
    "# Scrape to get news title and content\n",
    "news = soup.find('ul',class_='item_list')\n",
    "news_title = news.find('div',class_='content_title').text\n",
    "news_p = soup.find('ul',class_='item_list').find('div',class_='article_teaser_body').text\n",
    "browser.quit()\n",
    "# Splinter is very unstable for this website. Sometime the above code works and sometime not.\n",
    "# Therefore displaying the title an paragraph text below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perseverance Scientists Train for Mars in Nevada Desert'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Team members searched for signs of ancient microscopic life there, just as NASA's latest rover will on the Red Planet next year.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Nasa Mars News\n",
    "# url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# # Create soup object\n",
    "# html = requests.get(url).text\n",
    "# soup = create_soup(html)\n",
    "\n",
    "# # Scrape the webpage\n",
    "# news_title = soup.find('div',class_='content_title').a.text.strip()\n",
    "# news_p = soup.find('div', class_='image_and_description_container').text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping JPL Mars Space Images\n",
    "base_url = \"https://www.jpl.nasa.gov\"\n",
    "scrape_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n",
    "# Create Browser object\n",
    "browser = init_browser()\n",
    "browser.visit(scrape_url)\n",
    "\n",
    "# Create Soup object\n",
    "html = browser.html\n",
    "soup = create_soup(html)\n",
    "\n",
    "# Scrape the webpage\n",
    "featured_image_url = ( soup.find('article')['style']\n",
    "                      .replace('background-image: url(\\'','')\n",
    "                      .replace('\\');','')\n",
    "                     )# Using replace to extract only the URL\n",
    "\n",
    "# Build complete URL\n",
    "featured_image_url = base_url + featured_image_url\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Mars Weather twitter account - Did not work\n",
    "# scrape_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "# # Create Browser object\n",
    "# browser = init_browser()\n",
    "# browser.visit(scrape_url)\n",
    "\n",
    "# # Create Soup object\n",
    "# html = browser.html\n",
    "# soup = create_soup(html)\n",
    "\n",
    "# # Scrape the website - Assuming that if the text contains low,high and pressure it is related to weather\n",
    "# mars_weather = soup.find_all(string=re.compile('low.*high.*\\n.*\\npressure.*'))[0]\n",
    "\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Mars Weather twitter account\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "# Create soup object\n",
    "html = requests.get(url).text\n",
    "soup = create_soup(html)\n",
    "\n",
    "# Scrape the website - Assuming that if the text contains low,high and pressure it is related to weather\n",
    "mars_weather = soup.find_all(string=re.compile('low.*high.*\\n.*\\npressure.*'))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Mars Facts Webpage\n",
    "url = 'https://space-facts.com/mars/'\n",
    "\n",
    "# Create a DataFrame to read the HTML table\n",
    "mars_df = pd.read_html(url)[0]\n",
    "mars_df.columns = ['description','value']\n",
    "mars_df['description'] = mars_df['description'].str.replace(':','')\n",
    "\n",
    "# Create index on description column\n",
    "mars_df.set_index('description',inplace=False)\n",
    "\n",
    "# Convert the DataFrame to HTML table\n",
    "html_data = mars_df.to_html(index=False).replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping USGS Astrogeology site\n",
    "base_url = 'https://astrogeology.usgs.gov'\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Create soup object\n",
    "html = requests.get(url).text\n",
    "soup = create_soup(html)\n",
    "\n",
    "# Retrieve the items\n",
    "items = soup.find_all('div',class_='item')\n",
    "\n",
    "# Dictionary and list to retrieve the results of scraping\n",
    "title_url_dict = {}\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for item in items:\n",
    "    title = item.a.h3.text\n",
    "    img_url = base_url + item.a['href']\n",
    "    \n",
    "    # Create soup object for the image URL to scrape the webpage\n",
    "    html = requests.get(img_url).text\n",
    "    img_soup = create_soup(html)\n",
    "    \n",
    "    # Get the image url string from the src attribute of the image\n",
    "    full_img_url = base_url + img_soup.find('img',class_='wide-image')['src']\n",
    "    \n",
    "    # Update dictionary with the results\n",
    "    title_url_dict = {'title':title,'img_url':full_img_url}\n",
    "    \n",
    "    # Update list\n",
    "    hemisphere_image_urls.append(title_url_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('PythonData': conda)",
   "language": "python",
   "name": "python361064bitpythondataconda652045616be442ec915e78c0611d3213"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
